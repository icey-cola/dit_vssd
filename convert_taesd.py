"""
TAESD æƒé‡è½¬æ¢è„šæœ¬: PyTorch -> Flax
å°† PyTorch æ ¼å¼çš„ TAESD æƒé‡è½¬æ¢ä¸º Flax æ ¼å¼

ä½¿ç”¨æ–¹æ³•:
    python convert_taesd.py --input taesd_encoder.safetensors --output taesd_flax.msgpack
    
æˆ–è€…åœ¨è„šæœ¬ä¸­ç›´æ¥é…ç½® INPUT_PATH å’Œ OUTPUT_PATH
"""

import jax.numpy as jnp
from safetensors.flax import load_file
import flax
from flax.core.frozen_dict import freeze
from flax.traverse_util import unflatten_dict
import re
from pathlib import Path
import argparse

# === é»˜è®¤é…ç½® ===
# å¯ä»¥ç›´æ¥ä¿®æ”¹è¿™é‡Œï¼Œæˆ–è€…ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
INPUT_PATH = "/home/limingjia1999/dit-vssd/taesd_weights.safetensors"
OUTPUT_PATH = "/home/limingjia1999/dit-vssd/taesd_flax.msgpack"


def convert_key(key):
    """
    å°† PyTorch key æ˜ å°„åˆ° Flax key
    
    PyTorch TAESD ç¤ºä¾‹:
        encoder.layers.0.weight -> encoder_layers_0_kernel
        decoder.layers.5.conv.2.bias -> decoder_layers_5_conv_layers_2_bias
        
    ç‰¹æ®Šå¤„ç†:
        1. PyTorch "layers" å®¹å™¨åéœ€è¦è·³è¿‡
        2. Decoder çš„åºå·éœ€è¦+1ï¼Œå› ä¸º Flax ç¬¬0å±‚æ˜¯ Clamp (æ— å‚æ•°)
        
    Flax å‘½åè§„åˆ™:
        - è·³è¿‡ "layers" å®¹å™¨å
        - æ•°å­—ç´¢å¼•: "0" -> "layers_0"
        - Decoder ç´¢å¼•éœ€è¦+1
        - æƒé‡: weight -> kernel
        - åç½®: bias -> bias
    """
    # å…ˆå°† ".layers." æ›¿æ¢æ‰
    key = key.replace('.layers.', '.')
    
    parts = key.split('.')
    new_parts = []
    is_decoder = False
    
    for i, p in enumerate(parts):
        if p == 'decoder':
            is_decoder = True
            new_parts.append(p)
        elif p.isdigit():
            # æ•°å­—ç´¢å¼•å¤„ç†
            idx = int(p)
            # Decoder éœ€è¦+1 (å› ä¸º Flax çš„ layers_0 æ˜¯ Clamp)
            if is_decoder and i == 1:  # decoder çš„ç¬¬ä¸€ä¸ªæ•°å­—ç´¢å¼•
                idx += 1
            new_parts.append(f"layers_{idx}")
        else:
            new_parts.append(p)
            
    # é‡æ–°ç»„åˆ
    new_key = "_".join(new_parts)
    
    # å¤„ç†æƒé‡åç§°æ˜ å°„
    if new_key.endswith("_weight"):
        new_key = new_key.replace("_weight", "_kernel")

    return new_key


def convert_weights(path, verbose=True):
    """
    è½¬æ¢æƒé‡æ–‡ä»¶
    
    Args:
        path: è¾“å…¥ .safetensors è·¯å¾„
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
    Returns:
        è½¬æ¢åçš„ Flax å‚æ•°å­—å…¸ (æ‰å¹³æ ¼å¼)
    """
    pt_weights = load_file(path)
    flax_params = {}
    
    if verbose:
        print(f"\næ­£åœ¨è½¬æ¢: {path}")
        print(f"å‘ç° {len(pt_weights)} ä¸ªå‚æ•°")
        print("-" * 80)

    for key, tensor in pt_weights.items():
        # è·³è¿‡ä¸éœ€è¦çš„é”® (æ¯”å¦‚ PyTorch ç‰ˆæœ¬ä¿¡æ¯)
        if "num_batches_tracked" in key: 
            if verbose:
                print(f"è·³è¿‡: {key}")
            continue
            
        new_key = convert_key(key)
        
        # === ç»´åº¦è½¬ç½® (æ ¸å¿ƒ) ===
        # PyTorch Conv2d: [Out, In, H, W]
        # Flax Conv:      [H, W, In, Out]
        original_shape = tensor.shape
        if tensor.ndim == 4:
            tensor = jnp.transpose(tensor, (2, 3, 1, 0))
            
        # æ³¨æ„: Linear å±‚ TAESD æ²¡æœ‰ï¼Œå¦‚æœæœ‰éœ€å¤„ç† (1, 0) è½¬ç½®
        # å¦‚æœæ˜¯ 1D (bias) æˆ– å…¶ä»–ï¼Œä¿æŒä¸å˜
        
        flax_params[new_key] = tensor
        
        if verbose:
            shape_str = f"{original_shape} -> {tensor.shape}" if tensor.ndim == 4 else str(tensor.shape)
            print(f"{key:50s} -> {new_key:50s} | {shape_str}")

    if verbose:
        print("-" * 80)
        print(f"âœ… è½¬æ¢å®Œæˆï¼Œå…± {len(flax_params)} ä¸ªå‚æ•°")
        
    return flax_params


def save_msgpack(params, output_path, verbose=True):
    """
    ä¿å­˜ä¸º .msgpack æ ¼å¼
    
    Args:
        params: Flax å‚æ•°å­—å…¸ (æ‰å¹³æ ¼å¼ï¼Œkey æ˜¯å­—ç¬¦ä¸²)
        output_path: è¾“å‡ºè·¯å¾„
        verbose: æ˜¯å¦æ‰“å°ä¿¡æ¯
    """
    # å°†æ‰å¹³çš„å­—ç¬¦ä¸² key è½¬æ¢ä¸º tuple key
    # "encoder_layers_0_kernel" -> ("encoder", "layers_0", "kernel")
    tuple_dict = {}
    for key_str, value in params.items():
        # ç®€å•ç­–ç•¥ï¼šæŒ‰ä¸‹åˆ’çº¿åˆ†å‰²ï¼Œä½†ä¿æŠ¤ layers_æ•°å­— æ¨¡å¼
        import re
        # æ‰¾å‡ºæ‰€æœ‰ layers_æ•°å­— çš„ä½ç½®å¹¶æ›¿æ¢ä¸ºå ä½ç¬¦
        layer_indices = {}
        placeholder_key = key_str
        for match in re.finditer(r'layers_\d+', key_str):
            placeholder = f"__LAYER{len(layer_indices)}__"
            layer_indices[placeholder] = match.group()
            placeholder_key = placeholder_key.replace(match.group(), placeholder, 1)
        
        # ç°åœ¨å¯ä»¥å®‰å…¨åœ°åˆ†å‰²
        parts = placeholder_key.split('_')
        
        # æ¢å¤ layers_X
        parts = tuple(layer_indices.get(p, p) for p in parts)
        
        tuple_dict[parts] = value
    
    # ä½¿ç”¨ Flax çš„ unflatten_dict è½¬æ¢ä¸ºåµŒå¥—ç»“æ„
    nested_params = unflatten_dict(tuple_dict)
    
    # åºåˆ—åŒ–ä¸º msgpack
    msgpack_bytes = flax.serialization.to_bytes(nested_params)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # å†™å…¥æ–‡ä»¶
    with open(output_path, "wb") as f:
        f.write(msgpack_bytes)
    
    if verbose:
        file_size_mb = len(msgpack_bytes) / (1024 * 1024)
        print(f"\nâœ… ä¿å­˜æˆåŠŸ!")
        print(f"æ–‡ä»¶è·¯å¾„: {output_path}")
        print(f"æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB")


def convert_pth_to_safetensors(pth_path, safetensors_path):
    """
    å¯é€‰: å°† .pth/.bin è½¬æ¢ä¸º .safetensors
    éœ€è¦å®‰è£… PyTorch
    
    Args:
        pth_path: .pth æ–‡ä»¶è·¯å¾„
        safetensors_path: è¾“å‡º .safetensors è·¯å¾„
    """
    try:
        import torch
        from safetensors.torch import save_file
        
        state_dict = torch.load(pth_path, map_location='cpu')
        
        # å¦‚æœåŠ è½½çš„æ˜¯ checkpointï¼Œæå– state_dict
        if isinstance(state_dict, dict) and 'state_dict' in state_dict:
            state_dict = state_dict['state_dict']
        
        # ä¿å­˜ä¸º safetensors
        save_file(state_dict, safetensors_path)
        print(f"âœ… å·²å°† {pth_path} è½¬æ¢ä¸º {safetensors_path}")
        
    except ImportError:
        print("é”™è¯¯: éœ€è¦å®‰è£… PyTorch æ‰èƒ½è½¬æ¢ .pth æ–‡ä»¶")
        print("è¿è¡Œ: pip install torch safetensors")
        raise


def main():
    parser = argparse.ArgumentParser(
        description="å°† PyTorch TAESD æƒé‡è½¬æ¢ä¸º Flax æ ¼å¼"
    )
    parser.add_argument(
        "--input", "-i",
        type=str,
        default=INPUT_PATH,
        help="è¾“å…¥æ–‡ä»¶è·¯å¾„ (.safetensors æˆ– .pth)"
    )
    parser.add_argument(
        "--output", "-o",
        type=str,
        default=OUTPUT_PATH,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ (.msgpack)"
    )
    parser.add_argument(
        "--quiet", "-q",
        action="store_true",
        help="é™é»˜æ¨¡å¼ï¼Œä¸æ‰“å°è¯¦ç»†ä¿¡æ¯"
    )
    
    args = parser.parse_args()
    verbose = not args.quiet
    
    input_path = Path(args.input)
    output_path = Path(args.output)
    
    if verbose:
        print("=" * 80)
        print("TAESD æƒé‡è½¬æ¢: PyTorch -> Flax")
        print("=" * 80)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not input_path.exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {input_path}")
        return
    
    # å¦‚æœæ˜¯ .pth æ–‡ä»¶ï¼Œå…ˆè½¬æ¢ä¸º safetensors
    if input_path.suffix in ['.pth', '.bin']:
        if verbose:
            print(f"\næ£€æµ‹åˆ° PyTorch æ ¼å¼æ–‡ä»¶ï¼Œæ­£åœ¨è½¬æ¢ä¸º safetensors...")
        temp_safetensors = input_path.with_suffix('.safetensors')
        convert_pth_to_safetensors(input_path, temp_safetensors)
        input_path = temp_safetensors
    
    # æ‰§è¡Œè½¬æ¢
    try:
        flat_params = convert_weights(input_path, verbose=verbose)
        save_msgpack(flat_params, output_path, verbose=verbose)
        
        if verbose:
            print("\n" + "=" * 80)
            print("ğŸ‰ è½¬æ¢å®Œæˆ!")
            print("=" * 80)
            print(f"\nä½¿ç”¨æ–¹æ³•:")
            print(f"```python")
            print(f"import flax")
            print(f"from taesd_flax import FlaxTAESD")
            print(f"")
            print(f"# åŠ è½½æ¨¡å‹")
            print(f"model = FlaxTAESD(latent_channels=4)")
            print(f"")
            print(f"# åŠ è½½æƒé‡")
            print(f"with open('{output_path}', 'rb') as f:")
            print(f"    params = flax.serialization.from_bytes(None, f.read())")
            print(f"```")
        
    except Exception as e:
        print(f"\nâŒ è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        print("\næç¤º:")
        print("1. ç¡®ä¿è¾“å…¥æ–‡ä»¶æ˜¯ .safetensors æˆ– .pth æ ¼å¼")
        print("2. å¦‚æœæ˜¯ .pthï¼Œéœ€è¦å®‰è£… PyTorch: pip install torch")
        print("3. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æŸå")


if __name__ == "__main__":
    # å¯ä»¥ç›´æ¥è¿è¡Œæ­¤è„šæœ¬ï¼Œæˆ–ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
    # 
    # ç¤ºä¾‹ 1: ç›´æ¥è¿è¡Œï¼ˆä½¿ç”¨è„šæœ¬å†…çš„é»˜è®¤è·¯å¾„ï¼‰
    #   python convert_taesd.py
    #
    # ç¤ºä¾‹ 2: ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
    #   python convert_taesd.py --input path/to/taesd_encoder.safetensors --output output.msgpack
    #
    # ç¤ºä¾‹ 3: è½¬æ¢ decoder
    #   python convert_taesd.py -i taesd_decoder.pth -o taesd_decoder_flax.msgpack
    
    main()
